# Weather Forecast Application - Viva Notes

## Part 1: Project Overview

### Goal
The primary goal of this project is to predict future weather conditions (Temperature, Humidity, Wind Speed, Wind Direction) for multiple cities (Ahmedabad, Mumbai, Delhi, Bengaluru) based on historical hourly data. The predictions are made available through a web API and accessible via a simple frontend interface.

### Input Data
*   **Source:** Historical hourly weather data, likely obtained from sources like Meteostat (as suggested by potential data fetching scripts).
*   **Format:** CSV files, one per city (e.g., `ahmedabad.csv`).
    *   **Location:** `app/data/` directory.
*   **Key Features (Input):** Timestamp, Temperature (째C), Humidity (%), Wind Speed (km/h), Wind Direction (째). Other features might exist in raw data but are filtered during preprocessing.
*   **Target Variables:** The project aims to predict these four features:
    1.  Temperature (째C)
    2.  Humidity (%)
    3.  Wind Speed (km/h)
    4.  Wind Direction (째)
    *   **Defined:** In `app/utils/preprocess.py` via the `TARGET_FEATURES` list.

### Overall Workflow
1.  **Data Loading:** Read historical weather data from CSV files for a specific city.
    *   **Where:** `app/utils/preprocess.py` (function: `load_data`)
2.  **Preprocessing:** Clean the data, parse timestamps, and engineer features (lag features, time-based features).
    *   **Where:** `app/utils/preprocess.py` (function: `create_features`)
3.  **Model Training:** Train several machine learning models (LightGBM, XGBoost, CatBoost, ExtraTrees, HistGradientBoosting) for each city using the preprocessed data. Save the trained models.
    *   **Where:** `app/ml/train_models.py` (function: `train_all_models`)
4.  **Prediction:**
    *   Load a trained model (or use an ensemble).
    *   Load the latest historical data.
    *   Iteratively predict future weather conditions hour by hour, using previous predictions as input for subsequent steps.
    *   **Where (Single Model):** `app/ml/predict.py` (function: `make_predictions`)
    *   **Where (Ensemble):** `app/ml/ensemble.py` (function: `predict_ensemble`)
5.  **API:** Expose the prediction functionality through a FastAPI web server. The API endpoint (`/predict`) takes parameters like city, model choice, and forecast duration.
    *   **Where:** `app/main.py` (FastAPI app instance and `@app.get("/predict")` endpoint)
6.  **Frontend:** Provide a basic HTML/CSS/JavaScript interface for users to select options and view the forecasts fetched from the API.
    *   **Where:** `frontend/` directory (`index.html`, `style.css`, `script.js`)

## Part 2: Data Preprocessing (`app/utils/preprocess.py`)

### Loading (`load_data`)
*   **Where:** Defined and used within `app/utils/preprocess.py`. Called by `prepare_data_for_training` and `make_predictions`.
*   **How:** Uses `pandas.read_csv` to load data from the `app/data/` directory based on `city_name`. It then uses pandas datetime functions (`pd.to_datetime`), sorting (`sort_values`), index setting (`set_index`), and column selection.

### Feature Engineering (`create_features`)
*   **Where:** Defined and used within `app/utils/preprocess.py`. Called by `prepare_data_for_training` and `prepare_data_for_prediction`.
*   **How (Lag Features):** Iterates through `TARGET_FEATURES` and uses a nested loop from 1 to `LAG_FEATURES` (default 24). Inside the loop, `df[col].shift(lag)` creates a new column containing values from `lag` hours prior.
*   **How (Time Features):** Accesses the pandas DatetimeIndex attributes (`.dt.hour`, `.dt.dayofweek`, etc.) to create new columns directly from the timestamp index.
*   **How (Handling NaNs):** Uses `df.dropna(inplace=True)` to remove rows with any missing values generated by the `shift()` operation.

### Data Preparation (`prepare_data_for_training`, `prepare_data_for_prediction`)
*   **Where:** Defined in `app/utils/preprocess.py`.
    *   `prepare_data_for_training` is called by `train_all_models` in `app/ml/train_models.py`.
    *   `prepare_data_for_prediction` is called iteratively within `make_predictions` in `app/ml/predict.py`.
*   **How:** These functions orchestrate the loading and feature creation steps.
    *   `prepare_data_for_training` splits the full processed DataFrame into `X` (features) and `y` (targets) using `df.drop()`.
    *   `prepare_data_for_prediction` slices the *most recent* history (`df_history.iloc[-lag_features:]`), creates features for this slice, and returns only the *last row* of features (`df_feat.iloc[-1:].drop(...)`) needed for the immediate next prediction.

## Part 3: Model Training & Algorithms (`app/ml/train_models.py`)

### Goal
To train and save machine learning models capable of predicting the four `TARGET_FEATURES` based on the features generated during preprocessing. Models are trained separately for each city.

### Algorithms Used (`models_config` dictionary)
*   **Where Defined:** `app/ml/train_models.py`
*   **How Many Used:** 5 distinct base algorithms are defined. Since models are trained per city, and there are 4 cities (`CITIES` list), a total of 5 * 4 = **20 model files (`.pkl`)** are generated and saved in `app/models/`.
*   **List of Algorithms:**
    1.  **LightGBM (`lgb.LGBMRegressor`)**
    2.  **CatBoost (`cb.CatBoostRegressor`)**
    3.  **ExtraTrees (`sklearn.ensemble.ExtraTreesRegressor`)**
    4.  **XGBoost (`xgb.XGBRegressor`)**
    5.  **HistGradientBoosting (`sklearn.ensemble.HistGradientBoostingRegressor`)**

### Multi-Output Strategy
*   **Where Implemented:** `app/ml/train_models.py` within the `models_config` dictionary definition.
*   **How:** The `sklearn.multioutput.MultiOutputRegressor` wrapper is applied to LightGBM, CatBoost, XGBoost, and HistGradientBoosting because they natively only predict one target value. `ExtraTreesRegressor` handles multi-output directly.
*   **Why:** To enable models designed for single-target regression to predict all four weather features simultaneously by training an independent clone of the base model for each target.

### Training Loop (`train_all_models` function)
*   **Where:** Defined and executed if `app/ml/train_models.py` is run as the main script (`if __name__ == "__main__":`).
*   **How:**
    1.  Iterates through `CITIES`.
    2.  Calls `prepare_data_for_training` to get `X` and `y` for the current city.
    3.  Iterates through the `models_config` dictionary (model names and instantiated model objects).
    4.  Calls `model.fit(X, y)` to train the specific model on the city's data.
    5.  Uses `joblib.dump()` to serialize and save the trained `model` object to a `.pkl` file in the `app/models/` directory.

### Why These Algorithms? (The 5 Whys)
1.  **Why use tree-based ensembles (like Gradient Boosting, Random Forests/ExtraTrees)?**
    *   *Answer:* They are highly effective and often state-of-the-art for structured/tabular data prediction tasks, capturing complex non-linear relationships well. They generally require less feature scaling compared to linear models or neural networks.
2.  **Why use multiple different gradient boosting libraries (LightGBM, XGBoost, CatBoost, HistGradientBoosting)?**
    *   *Answer:* While based on similar principles, their implementations differ in terms of speed, memory usage, handling of data splits, regularization, and default parameter robustness. Including multiple variants increases the likelihood of finding a model that performs exceptionally well on this specific dataset and task without extensive hyperparameter tuning for each.
3.  **Why include ExtraTrees?**
    *   *Answer:* To incorporate a different ensemble philosophy (bagging with randomized splits vs. boosting). It adds diversity to the model pool, which can be beneficial for the ensemble average. It also natively supports multi-output regression, simplifying its use case here.
4.  **Why use established libraries (LightGBM, XGBoost, CatBoost, Scikit-learn)?**
    *   *Answer:* These libraries are well-documented, widely adopted, actively maintained, optimized for performance, and generally reliable. Using them avoids reinventing the wheel and leverages community testing and improvements.
5.  **Why train separate models for each city?**
    *   *Answer:* Weather patterns are geographically dependent. Training a specific model for each city allows the model to learn the unique local characteristics and temporal dependencies, which is expected to yield higher prediction accuracy compared to a single, more generalized model trained on data from all cities combined.

## Part 4: Prediction & API

### Prediction Logic (`app/ml/predict.py` - `make_predictions`)
*   **Where:** Defined in `app/ml/predict.py`, called by `predict_ensemble` and the `/predict` API endpoint in `app/main.py`.
*   **How (Iterative Forecasting - 1st How):**
    1.  Loads the specified model file using `joblib.load`.
    2.  Loads the full historical data using `load_data`.
    3.  Loops `hours_to_predict` times. In each loop:
        *   It prepares **one row** of input features using `prepare_data_for_prediction` based on the *most recent available data* (including previous predictions).
        *   It calls `model.predict()` on that single row to get the forecast for the next hour.
        *   It appends this new prediction back to the history data. This recursive process allows the model's own recent predictions to influence subsequent predictions.

### Ensemble Prediction (`app/ml/ensemble.py` - `predict_ensemble`)
*   **Where:** Defined in `app/ml/ensemble.py`, called by the `/predict` API endpoint in `app/main.py` when `model_name` is "Ensemble".
*   **How (Averaging - 2nd How):**
    1.  It iterates through the `BASE_MODEL_NAMES` list.
    2.  For each name, it calls `make_predictions` to get the full forecast (e.g., 48 hours) from that individual model.
    3.  It collects all these forecast DataFrames.
    4.  It uses `pd.concat` to stack these DataFrames.
    5.  It then uses `groupby(level=0).mean()` (or `groupby(df.index).mean()` after setting index) to calculate the average prediction for each target variable at each timestamp across all the base models.

### API (`app/main.py`)
*   **Where:** The main application logic resides in `app/main.py`.
*   **How:** Uses FastAPI decorators (`@app.get("/")`, `@app.get("/predict")`) to define URL paths and link them to asynchronous Python functions (`async def`). It uses FastAPI's `Query` for parameter definition and validation, `HTTPException` for error handling, and automatically handles JSON request/response serialization. `CORSMiddleware` is added to the app instance to handle browser security policies.

### Frontend (`frontend/`)
*   **Where:** Resides entirely within the `frontend/` directory.
*   **How:** Standard web technologies. `index.html` defines the structure, `style.css` defines the presentation, and `script.js` uses the browser's `fetch` API to asynchronously communicate with the FastAPI backend, sending user selections as query parameters and dynamically updating the HTML table with the results.